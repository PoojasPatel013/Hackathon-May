{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79398f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from .\\earthquake.csv: 1000 rows\n",
      "Loaded dataset from .\\flood.csv: 50000 rows\n",
      "Loaded dataset from .\\forestfires.csv: 517 rows\n",
      "Loaded dataset from .\\landslide.csv: 28 rows\n",
      "Loaded dataset from .\\storms.csv: 12230 rows\n",
      "Loaded dataset from .\\stroms.csv: 59228 rows\n",
      "Loaded dataset from .\\tsunami.csv: 2259 rows\n",
      "Loaded dataset from .\\volcano.csv: 877 rows\n",
      "Loaded dataset from .\\mix_all.xlsx: 26955 rows\n",
      "\n",
      "--- Feature Engineering Diagnostics ---\n",
      "Feature Matrix Shape: (153094, 14)\n",
      "Feature Columns: ['magnitude', 'eq_magnitude', 'Magnitude', 'depth', 'eq_depth', 'Depth', 'wind', 'wind_speed', 'Wind', 'tsunami', 'ts_intensity', 'Tsunami', 'sig', 'Significance', 'mmi', 'MMI', 'pressure', 'Pressure', 'temp', 'Temperature', 'area', 'Area', 'category', 'Category', 'total_damage_($mil)', 'damage_($mil)', 'Total Damage', 'houses_destroyed', 'Houses Destroyed', 'deaths', 'Total Deaths', 'injuries', 'Total Injuries']\n",
      "Risk Level Distribution:\n",
      "  Risk Level 0: 148104 samples\n",
      "  Risk Level 1: 3990 samples\n",
      "  Risk Level 2: 1000 samples\n",
      "Epoch 0: Train Loss 1.0525, Train Acc 0.0707, Test Loss 1.0345, Test Acc 0.0065\n",
      "Epoch 50: Train Loss 0.4803, Train Acc 0.9895, Test Loss 0.4468, Test Acc 0.9982\n",
      "Epoch 100: Train Loss 0.1028, Train Acc 0.9960, Test Loss 0.0733, Test Acc 0.9986\n",
      "Epoch 150: Train Loss 0.0290, Train Acc 0.9957, Test Loss 0.0137, Test Acc 0.9988\n",
      "Epoch 200: Train Loss 0.0157, Train Acc 0.9961, Test Loss 0.0057, Test Acc 0.9993\n",
      "Epoch 250: Train Loss 0.0114, Train Acc 0.9963, Test Loss 0.0035, Test Acc 0.9993\n",
      "Epoch 300: Train Loss 0.0092, Train Acc 0.9966, Test Loss 0.0022, Test Acc 0.9996\n",
      "Epoch 350: Train Loss 0.0077, Train Acc 0.9967, Test Loss 0.0018, Test Acc 0.9996\n",
      "Epoch 400: Train Loss 0.0067, Train Acc 0.9969, Test Loss 0.0011, Test Acc 0.9998\n",
      "Epoch 450: Train Loss 0.0063, Train Acc 0.9969, Test Loss 0.0009, Test Acc 0.9999\n",
      "Model components saved to disaster_risk_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class AdvancedDisasterRiskPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessing for disaster risk analysis\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframes):\n",
    "        \"\"\"\n",
    "        Initialize preprocessor with multiple dataframes\n",
    "        \n",
    "        Args:\n",
    "            dataframes (list): List of input dataframes\n",
    "        \"\"\"\n",
    "        self.dataframes = dataframes\n",
    "        self.processed_features = None\n",
    "        self.risk_labels = None\n",
    "    \n",
    "    def select_and_engineer_features(self):\n",
    "        \"\"\"\n",
    "        Advanced feature selection and engineering across multiple datasets\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Processed features and risk labels\n",
    "        \"\"\"\n",
    "        # Comprehensive risk-related features with fallback columns\n",
    "        RISK_FEATURE_GROUPS = [\n",
    "            ['magnitude', 'eq_magnitude', 'Magnitude'],\n",
    "            ['depth', 'eq_depth', 'Depth'],\n",
    "            ['wind', 'wind_speed', 'Wind'],\n",
    "            ['tsunami', 'ts_intensity', 'Tsunami'],\n",
    "            ['sig', 'Significance'],\n",
    "            ['mmi', 'MMI'],\n",
    "            ['pressure', 'Pressure'],\n",
    "            ['temp', 'Temperature'],\n",
    "            ['area', 'Area'],\n",
    "            ['category', 'Category']\n",
    "        ]\n",
    "        \n",
    "        # Damage and impact features with fallback columns\n",
    "        DAMAGE_FEATURE_GROUPS = [\n",
    "            ['total_damage_($mil)', 'damage_($mil)', 'Total Damage'],\n",
    "            ['houses_destroyed', 'Houses Destroyed'],\n",
    "            ['deaths', 'Total Deaths'],\n",
    "            ['injuries', 'Total Injuries']\n",
    "        ]\n",
    "        \n",
    "        # Preprocessing steps\n",
    "        def safe_feature_extraction(row, feature_group):\n",
    "            \"\"\"\n",
    "            Safely extract numeric value from multiple possible columns\n",
    "            \n",
    "            Args:\n",
    "                row (pd.Series): DataFrame row\n",
    "                feature_group (list): Possible column names\n",
    "            \n",
    "            Returns:\n",
    "                float: Extracted numeric value\n",
    "            \"\"\"\n",
    "            for feature in feature_group:\n",
    "                # Try to extract value\n",
    "                value = row.get(feature, np.nan)\n",
    "                \n",
    "                # Convert to numeric\n",
    "                try:\n",
    "                    numeric_value = pd.to_numeric(value, errors='coerce')\n",
    "                    if not pd.isna(numeric_value):\n",
    "                        return numeric_value\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return 0.0\n",
    "        \n",
    "        # Prepare feature matrix\n",
    "        feature_matrix = []\n",
    "        risk_labels = []\n",
    "        \n",
    "        # Process each dataframe\n",
    "        for df in self.dataframes:\n",
    "            # Process each row in the dataframe\n",
    "            for _, row in df.iterrows():\n",
    "                # Extract risk-related features\n",
    "                features = []\n",
    "                \n",
    "                # Extract features from groups\n",
    "                for feature_group in RISK_FEATURE_GROUPS + DAMAGE_FEATURE_GROUPS:\n",
    "                    features.append(safe_feature_extraction(row, feature_group))\n",
    "                \n",
    "                # Risk scoring mechanism\n",
    "                def calculate_risk_score(features):\n",
    "                    \"\"\"\n",
    "                    Comprehensive risk scoring\n",
    "                    \n",
    "                    Args:\n",
    "                        features (list): Input features\n",
    "                    \n",
    "                    Returns:\n",
    "                        int: Risk category\n",
    "                    \"\"\"\n",
    "                    # Weighted risk calculation\n",
    "                    risk_weights = {\n",
    "                        'magnitude': 0.2,\n",
    "                        'depth': 0.1,\n",
    "                        'wind': 0.1,\n",
    "                        'tsunami': 0.1,\n",
    "                        'sig': 0.1,\n",
    "                        'mmi': 0.1,\n",
    "                        'pressure': 0.1,\n",
    "                        'temp': 0.1,\n",
    "                        'area': 0.05,\n",
    "                        'category': 0.05,\n",
    "                        'damage': 0.1\n",
    "                    }\n",
    "                    \n",
    "                    # Calculate total risk\n",
    "                    total_risk = 0\n",
    "                    for i, (feature_group, weight_key) in enumerate(\n",
    "                        zip(RISK_FEATURE_GROUPS + DAMAGE_FEATURE_GROUPS, \n",
    "                            list(risk_weights.keys()))\n",
    "                    ):\n",
    "                        # Normalize feature\n",
    "                        if len(features) > i:\n",
    "                            normalized_value = min(max(features[i] / (max(features[i], 1)), 0), 1)\n",
    "                            feature_weight = risk_weights.get(weight_key, 0.05)\n",
    "                            total_risk += normalized_value * feature_weight\n",
    "                    \n",
    "                    # Risk categorization\n",
    "                    if total_risk <= 0.2:\n",
    "                        return 0  # Very Low Risk\n",
    "                    elif total_risk <= 0.4:\n",
    "                        return 1  # Low Risk\n",
    "                    elif total_risk <= 0.6:\n",
    "                        return 2  # Moderate Risk\n",
    "                    elif total_risk <= 0.8:\n",
    "                        return 3  # High Risk\n",
    "                    else:\n",
    "                        return 4  # Extreme Risk\n",
    "                \n",
    "                # Calculate risk label\n",
    "                risk_label = calculate_risk_score(features)\n",
    "                \n",
    "                # Append features and label\n",
    "                feature_matrix.append(features)\n",
    "                risk_labels.append(risk_label)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(feature_matrix, dtype=np.float32)\n",
    "        y = np.array(risk_labels, dtype=np.int32)\n",
    "        \n",
    "        # Impute any remaining missing values\n",
    "        numeric_imputer = SimpleImputer(strategy='median')\n",
    "        X = numeric_imputer.fit_transform(X)\n",
    "        \n",
    "        # Store processed data\n",
    "        self.processed_features = X\n",
    "        self.risk_labels = y\n",
    "        \n",
    "        # Logging\n",
    "        print(\"\\n--- Feature Engineering Diagnostics ---\")\n",
    "        print(f\"Feature Matrix Shape: {X.shape}\")\n",
    "        print(\"Feature Columns:\", \n",
    "              [col for group in RISK_FEATURE_GROUPS + DAMAGE_FEATURE_GROUPS for col in group])\n",
    "        print(\"Risk Level Distribution:\")\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        for label, count in zip(unique, counts):\n",
    "            print(f\"  Risk Level {label}: {count} samples\")\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "class DisasterRiskNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Sophisticated Neural Network for Disaster Risk Prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extraction with adaptive architecture\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, max(64, input_dim * 2)),\n",
    "            nn.BatchNorm1d(max(64, input_dim * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(max(64, input_dim * 2), max(64, input_dim * 2)),\n",
    "            nn.BatchNorm1d(max(64, input_dim * 2)),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(max(64, input_dim * 2), 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "def train_advanced_disaster_model(X, y, epochs=500):\n",
    "    \"\"\"\n",
    "    Comprehensive training pipeline with advanced techniques\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Input features\n",
    "        y (np.array): Target labels\n",
    "        epochs (int): Number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Model, scaler, label encoder, and training diagnostics\n",
    "    \"\"\"\n",
    "    # Preprocessing\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Train-test split with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_encoded, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    y_test_tensor = torch.LongTensor(y_test)\n",
    "    \n",
    "    # Model initialization\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    model = DisasterRiskNetwork(input_dim=X.shape[1], num_classes=num_classes)\n",
    "    \n",
    "    # Loss and Optimizer with gradient clipping\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=20\n",
    "    )\n",
    "    \n",
    "    # Training diagnostics\n",
    "    diagnostics = {\n",
    "        'train_loss': [], 'test_loss': [],\n",
    "        'train_accuracy': [], 'test_accuracy': []\n",
    "    }\n",
    "    \n",
    "    # Training loop with advanced error handling\n",
    "    for epoch in range(epochs):\n",
    "        try:\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            train_outputs = model(X_train_tensor)\n",
    "            train_loss = criterion(train_outputs, y_train_tensor)\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(X_test_tensor)\n",
    "                test_loss = criterion(test_outputs, y_test_tensor)\n",
    "            \n",
    "            # Calculate accuracies\n",
    "            train_pred = torch.argmax(train_outputs, dim=1)\n",
    "            test_pred = torch.argmax(test_outputs, dim=1)\n",
    "            \n",
    "            train_accuracy = (train_pred == y_train_tensor).float().mean()\n",
    "            test_accuracy = (test_pred == y_test_tensor).float().mean()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(test_loss)\n",
    "            \n",
    "            # Record diagnostics\n",
    "            diagnostics['train_loss'].append(train_loss.item())\n",
    "            diagnostics['test_loss'].append(test_loss.item())\n",
    "            diagnostics['train_accuracy'].append(train_accuracy.item())\n",
    "            diagnostics['test_accuracy'].append(test_accuracy.item())\n",
    "            \n",
    "            # Periodic logging\n",
    "            if epoch % 50 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}: \"\n",
    "                    f\"Train Loss {train_loss.item():.4f}, \"\n",
    "                    f\"Train Acc {train_accuracy.item():.4f}, \"\n",
    "                    f\"Test Loss {test_loss.item():.4f}, \"\n",
    "                    f\"Test Acc {test_accuracy.item():.4f}\"\n",
    "                )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in epoch {epoch}: {e}\")\n",
    "    \n",
    "    return model, scaler, label_encoder, diagnostics\n",
    "\n",
    "def save_model_components(model, scaler, label_encoder, save_path='disaster_risk_model.pkl'):\n",
    "    \"\"\"\n",
    "    Save model components for deployment\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained neural network\n",
    "        scaler (StandardScaler): Feature scaler\n",
    "        label_encoder (LabelEncoder): Label encoder\n",
    "        save_path (str): Path to save model components\n",
    "    \"\"\"\n",
    "    # Prepare model for saving\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Create saving dictionary\n",
    "    save_dict = {\n",
    "        'model_state': model.state_dict(),\n",
    "        'model_class': model.__class__,\n",
    "        'input_dim': model.feature_extractor[0].in_features,\n",
    "        'num_classes': model.classifier[-1].out_features,\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': label_encoder,\n",
    "        'feature_names': [\n",
    "            # Risk-related features\n",
    "            'Magnitude', 'Depth', 'Wind Speed', 'Tsunami Intensity', \n",
    "            'Significance', 'MMI', 'Pressure', 'Temperature', \n",
    "            'Area', 'Category', \n",
    "            \n",
    "            # Damage features\n",
    "            'Total Damage ($mil)', 'Houses Destroyed', \n",
    "            'Deaths', 'Injuries'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save to pickle\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "    \n",
    "    print(f\"Model components saved to {save_path}\")\n",
    "\n",
    "def load_all_datasets(directory='.'):\n",
    "    \"\"\"\n",
    "    Load all CSV and Excel files from a directory\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory to search for datasets\n",
    "    \n",
    "    Returns:\n",
    "        list: List of loaded dataframes\n",
    "    \"\"\"\n",
    "    # Find all CSV and Excel files\n",
    "    csv_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "    excel_files = glob.glob(os.path.join(directory, '*.xlsx'))\n",
    "    \n",
    "    # Combine file lists\n",
    "    all_files = csv_files + excel_files\n",
    "    \n",
    "    # Load datasets\n",
    "    dataframes = []\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            # Skip certain files\n",
    "            if 'diagnostics' in file_path.lower():\n",
    "                continue\n",
    "            \n",
    "            # Load based on file extension\n",
    "            if file_path.endswith('.csv'):\n",
    "                df = pd.read_csv(file_path)\n",
    "            else:\n",
    "                df = pd.read_excel(file_path)\n",
    "            \n",
    "            print(f\"Loaded dataset from {file_path}: {len(df)} rows\")\n",
    "            dataframes.append(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Comprehensive disaster risk prediction pipeline\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load all datasets from current directory\n",
    "        dataframes = load_all_datasets()\n",
    "        \n",
    "        # Check if datasets were loaded\n",
    "        if not dataframes:\n",
    "            raise ValueError(\"No datasets found. Please ensure CSV or XLSX files are present.\")\n",
    "        \n",
    "        # Initialize preprocessor with all datasets\n",
    "        preprocessor = AdvancedDisasterRiskPreprocessor(dataframes)\n",
    "        \n",
    "        # Extract and engineer features\n",
    "        X, y = preprocessor.select_and_engineer_features()\n",
    "        \n",
    "        # Train advanced model\n",
    "        model, scaler, label_encoder, training_diagnostics = train_advanced_disaster_model(X, y)\n",
    "        \n",
    "        # Save model components for deployment\n",
    "        save_model_components(\n",
    "            model, \n",
    "            scaler, \n",
    "            label_encoder, \n",
    "            save_path='disaster_risk_model.pkl'\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(\"An error occurred:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d256fe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from .\\earthquake.csv: 1000 rows\n",
      "Loaded dataset from .\\flood.csv: 50000 rows\n",
      "Loaded dataset from .\\forestfires.csv: 517 rows\n",
      "Loaded dataset from .\\landslide.csv: 28 rows\n",
      "Loaded dataset from .\\storms.csv: 12230 rows\n",
      "Loaded dataset from .\\stroms.csv: 59228 rows\n",
      "Loaded dataset from .\\tsunami.csv: 2259 rows\n",
      "Loaded dataset from .\\volcano.csv: 877 rows\n",
      "Loaded dataset from .\\mix_all.xlsx: 26955 rows\n",
      "\n",
      "--- Feature Engineering Diagnostics ---\n",
      "Feature Matrix Shape: (153094, 17)\n",
      "Feature Columns: ['magnitude', 'eq_magnitude', 'Magnitude', 'Mag', 'depth', 'eq_depth', 'Depth', 'wind', 'wind_speed', 'Wind Speed', 'WindSpeed', 'hurricane_category', 'Category', 'tsunami', 'ts_intensity', 'Tsunami Intensity', 'TsunamiHeight']\n",
      "Risk Level Distribution:\n",
      "  Risk Level 0: 150237 samples\n",
      "  Risk Level 1: 1857 samples\n",
      "  Risk Level 2: 826 samples\n",
      "  Risk Level 3: 174 samples\n",
      "Epoch 0: Train Loss 1.2750, Train Acc 0.9448, Test Loss 1.2572, Test Acc 0.9820\n",
      "Epoch 50: Train Loss 0.7233, Train Acc 0.9841, Test Loss 0.6821, Test Acc 0.9844\n",
      "Epoch 100: Train Loss 0.2429, Train Acc 0.9888, Test Loss 0.1794, Test Acc 0.9921\n",
      "Epoch 150: Train Loss 0.0923, Train Acc 0.9912, Test Loss 0.0461, Test Acc 0.9931\n",
      "Epoch 200: Train Loss 0.0473, Train Acc 0.9922, Test Loss 0.0206, Test Acc 0.9942\n",
      "Epoch 250: Train Loss 0.0332, Train Acc 0.9928, Test Loss 0.0147, Test Acc 0.9945\n",
      "Epoch 300: Train Loss 0.0268, Train Acc 0.9928, Test Loss 0.0124, Test Acc 0.9951\n",
      "Epoch 350: Train Loss 0.0222, Train Acc 0.9934, Test Loss 0.0108, Test Acc 0.9952\n",
      "Epoch 400: Train Loss 0.0197, Train Acc 0.9939, Test Loss 0.0100, Test Acc 0.9955\n",
      "Epoch 450: Train Loss 0.0178, Train Acc 0.9942, Test Loss 0.0097, Test Acc 0.9955\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     30048\n",
      "           1       0.77      0.90      0.83       371\n",
      "           2       1.00      1.00      1.00       165\n",
      "           3       1.00      1.00      1.00        35\n",
      "\n",
      "    accuracy                           1.00     30619\n",
      "   macro avg       0.94      0.97      0.96     30619\n",
      "weighted avg       1.00      1.00      1.00     30619\n",
      "\n",
      "Model components saved to risk_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    precision_recall_curve, \n",
    "    roc_curve, \n",
    "    auc\n",
    ")\n",
    "\n",
    "class ComprehensiveDisasterRiskPreprocessor:\n",
    "    \"\"\"\n",
    "    Advanced data preprocessing for multi-disaster risk analysis\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframes):\n",
    "        \"\"\"\n",
    "        Initialize preprocessor with multiple dataframes\n",
    "        \n",
    "        Args:\n",
    "            dataframes (list): List of input dataframes\n",
    "        \"\"\"\n",
    "        self.dataframes = dataframes\n",
    "        self.processed_features = None\n",
    "        self.risk_labels = None\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def _get_comprehensive_feature_groups(self):\n",
    "        \"\"\"\n",
    "        Generate comprehensive feature groups across all datasets\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Risk and damage feature groups\n",
    "        \"\"\"\n",
    "        # Comprehensive risk-related features with fallback columns\n",
    "        RISK_FEATURE_GROUPS = [\n",
    "            # Earthquake features\n",
    "            ['magnitude', 'eq_magnitude', 'Magnitude', 'Mag'],\n",
    "            ['depth', 'eq_depth', 'Depth'],\n",
    "            \n",
    "            # Wind and storm features\n",
    "            ['wind', 'wind_speed', 'Wind Speed', 'WindSpeed'],\n",
    "            ['hurricane_category', 'Category'],\n",
    "            \n",
    "            # Tsunami features\n",
    "            ['tsunami', 'ts_intensity', 'Tsunami Intensity', 'TsunamiHeight'],\n",
    "            \n",
    "            # General disaster indicators\n",
    "            ['sig', 'Significance', 'SignificanceLevel'],\n",
    "            ['mmi', 'MMI', 'Modified Mercalli Intensity'],\n",
    "            \n",
    "            # Environmental features\n",
    "            ['pressure', 'Pressure', 'AtmosphericPressure'],\n",
    "            ['temp', 'Temperature', 'AirTemperature'],\n",
    "            ['area', 'Area', 'AffectedArea'],\n",
    "            \n",
    "            # Location and geographic features\n",
    "            ['latitude', 'Latitude'],\n",
    "            ['longitude', 'Longitude']\n",
    "        ]\n",
    "        \n",
    "        # Damage and impact features with fallback columns\n",
    "        DAMAGE_FEATURE_GROUPS = [\n",
    "            ['total_damage_($mil)', 'damage_($mil)', 'Total Damage', 'TotalDamage'],\n",
    "            ['houses_destroyed', 'Houses Destroyed', 'BuildingsDestroyed'],\n",
    "            ['deaths', 'Total Deaths', 'Fatalities'],\n",
    "            ['injuries', 'Total Injuries', 'Wounded'],\n",
    "            ['economic_loss', 'Economic Loss', 'EconomicImpact']\n",
    "        ]\n",
    "        \n",
    "        return RISK_FEATURE_GROUPS, DAMAGE_FEATURE_GROUPS\n",
    "    \n",
    "    def select_and_engineer_features(self):\n",
    "        \"\"\"\n",
    "        Advanced feature selection and engineering across multiple datasets\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Processed features and risk labels\n",
    "        \"\"\"\n",
    "        # Get feature groups\n",
    "        RISK_FEATURE_GROUPS, DAMAGE_FEATURE_GROUPS = self._get_comprehensive_feature_groups()\n",
    "        \n",
    "        # Preprocessing steps\n",
    "        def safe_feature_extraction(row, feature_group):\n",
    "            \"\"\"\n",
    "            Safely extract numeric value from multiple possible columns\n",
    "            \n",
    "            Args:\n",
    "                row (pd.Series): DataFrame row\n",
    "                feature_group (list): Possible column names\n",
    "            \n",
    "            Returns:\n",
    "                float: Extracted numeric value\n",
    "            \"\"\"\n",
    "            for feature in feature_group:\n",
    "                # Try to extract value\n",
    "                value = row.get(feature, np.nan)\n",
    "                \n",
    "                # Convert to numeric\n",
    "                try:\n",
    "                    numeric_value = pd.to_numeric(value, errors='coerce')\n",
    "                    if not pd.isna(numeric_value):\n",
    "                        return numeric_value\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return 0.0\n",
    "        \n",
    "        # Prepare feature matrix\n",
    "        feature_matrix = []\n",
    "        risk_labels = []\n",
    "        \n",
    "        # Process each dataframe\n",
    "        for df in self.dataframes:\n",
    "            # Process each row in the dataframe\n",
    "            for _, row in df.iterrows():\n",
    "                # Extract risk-related features\n",
    "                features = []\n",
    "                \n",
    "                # Extract features from groups\n",
    "                feature_groups = RISK_FEATURE_GROUPS + DAMAGE_FEATURE_GROUPS\n",
    "                for feature_group in feature_groups:\n",
    "                    features.append(safe_feature_extraction(row, feature_group))\n",
    "                \n",
    "                # Risk scoring mechanism\n",
    "                def calculate_risk_score(features):\n",
    "                    \"\"\"\n",
    "                    Comprehensive risk scoring\n",
    "                    \n",
    "                    Args:\n",
    "                        features (list): Input features\n",
    "                    \n",
    "                    Returns:\n",
    "                        int: Risk category\n",
    "                    \"\"\"\n",
    "                    # Weighted risk calculation\n",
    "                    risk_weights = {\n",
    "                        'magnitude': 0.15,\n",
    "                        'depth': 0.1,\n",
    "                        'wind': 0.1,\n",
    "                        'hurricane': 0.1,\n",
    "                        'tsunami': 0.1,\n",
    "                        'sig': 0.1,\n",
    "                        'mmi': 0.1,\n",
    "                        'pressure': 0.05,\n",
    "                        'temp': 0.05,\n",
    "                        'area': 0.05,\n",
    "                        'location': 0.05,\n",
    "                        'damage': 0.1,\n",
    "                        'economic': 0.05\n",
    "                    }\n",
    "                    \n",
    "                    # Calculate total risk\n",
    "                    total_risk = 0\n",
    "                    for i, (feature_group, weight_key) in enumerate(\n",
    "                        zip(feature_groups, \n",
    "                            list(risk_weights.keys()))\n",
    "                    ):\n",
    "                        # Normalize feature\n",
    "                        if len(features) > i:\n",
    "                            normalized_value = min(max(features[i] / (max(features[i], 1)), 0), 1)\n",
    "                            feature_weight = risk_weights.get(weight_key, 0.05)\n",
    "                            total_risk += normalized_value * feature_weight\n",
    "                    \n",
    "                    # Risk categorization\n",
    "                    if total_risk <= 0.2:\n",
    "                        return 0  # Very Low Risk\n",
    "                    elif total_risk <= 0.4:\n",
    "                        return 1  # Low Risk\n",
    "                    elif total_risk <= 0.6:\n",
    "                        return 2  # Moderate Risk\n",
    "                    elif total_risk <= 0.8:\n",
    "                        return 3  # High Risk\n",
    "                    else:\n",
    "                        return 4  # Extreme Risk\n",
    "                \n",
    "                # Calculate risk label\n",
    "                risk_label = calculate_risk_score(features)\n",
    "                \n",
    "                # Append features and label\n",
    "                feature_matrix.append(features)\n",
    "                risk_labels.append(risk_label)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(feature_matrix, dtype=np.float32)\n",
    "        y = np.array(risk_labels, dtype=np.int32)\n",
    "        \n",
    "        # Impute any remaining missing values\n",
    "        numeric_imputer = SimpleImputer(strategy='median')\n",
    "        X = numeric_imputer.fit_transform(X)\n",
    "        \n",
    "        # Store processed data\n",
    "        self.processed_features = X\n",
    "        self.risk_labels = y\n",
    "        \n",
    "        # Generate feature names\n",
    "        self.feature_names = [\n",
    "            col for group in RISK_FEATURE_GROUPS + DAMAGE_FEATURE_GROUPS \n",
    "            for col in group\n",
    "        ][:X.shape[1]]\n",
    "        \n",
    "        # Logging\n",
    "        print(\"\\n--- Feature Engineering Diagnostics ---\")\n",
    "        print(f\"Feature Matrix Shape: {X.shape}\")\n",
    "        print(\"Feature Columns:\", self.feature_names)\n",
    "        print(\"Risk Level Distribution:\")\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        for label, count in zip(unique, counts):\n",
    "            print(f\"  Risk Level {label}: {count} samples\")\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "class AdvancedDisasterRiskNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Sophisticated Neural Network for Comprehensive Disaster Risk Prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extraction with adaptive architecture\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, max(64, input_dim * 2)),\n",
    "            nn.BatchNorm1d(max(64, input_dim * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(max(64, input_dim * 2), max(64, input_dim * 2)),\n",
    "            nn.BatchNorm1d(max(64, input_dim * 2)),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(max(64, input_dim * 2), 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "def train_advanced_disaster_model(X, y, epochs=500):\n",
    "    \"\"\"\n",
    "    Comprehensive training pipeline with advanced techniques\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Input features\n",
    "        y (np.array): Target labels\n",
    "        epochs (int): Number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Model, scaler, label encoder, and training diagnostics\n",
    "    \"\"\"\n",
    "    # Preprocessing\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Train-test split with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_encoded, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    y_test_tensor = torch.LongTensor(y_test)\n",
    "    \n",
    "    # Model initialization\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    model = AdvancedDisasterRiskNetwork(input_dim=X.shape[1], num_classes=num_classes)\n",
    "    \n",
    "    # Loss and Optimizer with gradient clipping\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=20\n",
    "    )\n",
    "    \n",
    "    # Training diagnostics\n",
    "    diagnostics = {\n",
    "        'train_loss': [], 'test_loss': [],\n",
    "        'train_accuracy': [], 'test_accuracy': [],\n",
    "        'y_true': [], 'y_pred': []\n",
    "    }\n",
    "    \n",
    "    # Training loop with advanced error handling\n",
    "    for epoch in range(epochs):\n",
    "        try:\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            train_outputs = model(X_train_tensor)\n",
    "            train_loss = criterion(train_outputs, y_train_tensor)\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(X_test_tensor)\n",
    "                test_loss = criterion(test_outputs, y_test_tensor)\n",
    "            \n",
    "            # Calculate accuracies\n",
    "            train_pred = torch.argmax(train_outputs, dim=1)\n",
    "            test_pred = torch.argmax(test_outputs, dim=1)\n",
    "            \n",
    "            train_accuracy = (train_pred == y_train_tensor).float().mean()\n",
    "            test_accuracy = (test_pred == y_test_tensor).float().mean()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(test_loss)\n",
    "            \n",
    "            # Record diagnostics\n",
    "            diagnostics['train_loss'].append(train_loss.item())\n",
    "            diagnostics['test_loss'].append(test_loss.item())\n",
    "            diagnostics['train_accuracy'].append(train_accuracy.item())\n",
    "            diagnostics['test_accuracy'].append(test_accuracy.item())\n",
    "            \n",
    "            # Store predictions for final analysis\n",
    "            if epoch == epochs - 1:\n",
    "                diagnostics['y_true'] = y_test_tensor.numpy()\n",
    "                diagnostics['y_pred'] = test_pred.numpy()\n",
    "            \n",
    "            # Periodic logging\n",
    "            if epoch % 50 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}: \"\n",
    "                    f\"Train Loss {train_loss.item():.4f}, \"\n",
    "                    f\"Train Acc {train_accuracy.item():.4f}, \"\n",
    "                    f\"Test Loss {test_loss.item():.4f}, \"\n",
    "                    f\"Test Acc {test_accuracy.item():.4f}\"\n",
    "                )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in epoch {epoch}: {e}\")\n",
    "    \n",
    "    return model, scaler, label_encoder, diagnostics\n",
    "\n",
    "def generate_comprehensive_plots(diagnostics, preprocessor, model, X_test, y_test, label_encoder):\n",
    "    \"\"\"\n",
    "    Generate comprehensive visualization plots\n",
    "    \n",
    "    Args:\n",
    "        diagnostics (dict): Training diagnostics\n",
    "        preprocessor (ComprehensiveDisasterRiskPreprocessor): Feature preprocessor\n",
    "        model (nn.Module): Trained neural network\n",
    "        X_test (np.array): Test features\n",
    "        y_test (np.array): Test labels\n",
    "        label_encoder (LabelEncoder): Label encoder\n",
    "    \"\"\"\n",
    "    # Create a figure with multiple subplots\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Training and Test Loss\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(diagnostics['train_loss'], label='Train Loss')\n",
    "    plt.plot(diagnostics['test_loss'], label='Test Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 2. Training and Test Accuracy\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(diagnostics['train_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(diagnostics['test_accuracy'], label='Test Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 3. Confusion Matrix\n",
    "    plt.subplot(2, 3, 3)\n",
    "    y_true = diagnostics['y_true']\n",
    "    y_pred = diagnostics['y_pred']\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    \n",
    "    # 4. Feature Importance Visualization\n",
    "    plt.subplot(2, 3, 4)\n",
    "    feature_importance = np.abs(model.feature_extractor[0].weight.detach().numpy()).mean(axis=0)\n",
    "    feature_names = preprocessor.feature_names[:len(feature_importance)]\n",
    "    plt.bar(feature_names, feature_importance)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    \n",
    "    # 5. ROC Curve with One-vs-Rest approach\n",
    "    plt.subplot(2, 3, 5)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_pred_proba = torch.softmax(model(X_test_tensor), dim=1).detach().numpy()\n",
    "    n_classes = y_pred_proba.shape[1]\n",
    "    \n",
    "    # One-vs-Rest ROC\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    for i in range(n_classes):\n",
    "        # Binarize the output for this class\n",
    "        y_true_bin = (y_test == i).astype(int)\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin, y_pred_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, \n",
    "                 label=f'ROC (class {label_encoder.inverse_transform([i])[0]}, AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # 6. Precision-Recall Curve with One-vs-Rest approach\n",
    "    plt.subplot(2, 3, 6)\n",
    "    for i in range(n_classes):\n",
    "        # Binarize the output for this class\n",
    "        y_true_bin = (y_test == i).astype(int)\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin, y_pred_proba[:, i])\n",
    "        plt.plot(recall, precision, \n",
    "                 label=f'PR (class {label_encoder.inverse_transform([i])[0]})')\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('disaster_risk_analysis_plots.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(\n",
    "        y_true, \n",
    "        y_pred, \n",
    "        target_names=[str(label) for label in label_encoder.classes_]\n",
    "    ))\n",
    "\n",
    "def save_model_components(model, scaler, label_encoder, preprocessor, save_path='disaster_risk_model.pkl'):\n",
    "    \"\"\"\n",
    "    Save model components for deployment\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained neural network\n",
    "        scaler (StandardScaler): Feature scaler\n",
    "        label_encoder (LabelEncoder): Label encoder\n",
    "        preprocessor (ComprehensiveDisasterRiskPreprocessor): Feature preprocessor\n",
    "        save_path (str): Path to save model components\n",
    "    \"\"\"\n",
    "    # Prepare model for saving\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Create saving dictionary\n",
    "    save_dict = {\n",
    "        'model_state': model.state_dict(),\n",
    "        'model_class': model.__class__,\n",
    "        'input_dim': model.feature_extractor[0].in_features,\n",
    "        'num_classes': model.classifier[-1].out_features,\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': label_encoder,\n",
    "        'feature_names': preprocessor.feature_names\n",
    "    }\n",
    "    \n",
    "    # Save to pickle\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "    \n",
    "    print(f\"Model components saved to {save_path}\")\n",
    "\n",
    "def load_all_datasets(directory='.'):\n",
    "    \"\"\"\n",
    "    Load all CSV and Excel files from a directory\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory to search for datasets\n",
    "    \n",
    "    Returns:\n",
    "        list: List of loaded dataframes\n",
    "    \"\"\"\n",
    "    # Find all CSV and Excel files\n",
    "    csv_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "    excel_files = glob.glob(os.path.join(directory, '*.xlsx'))\n",
    "    \n",
    "    # Combine file lists\n",
    "    all_files = csv_files + excel_files\n",
    "    \n",
    "    # Load datasets\n",
    "    dataframes = []\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            # Skip certain files\n",
    "            if 'diagnostics' in file_path.lower():\n",
    "                continue\n",
    "            \n",
    "            # Load based on file extension\n",
    "            if file_path.endswith('.csv'):\n",
    "                df = pd.read_csv(file_path)\n",
    "            else:\n",
    "                df = pd.read_excel(file_path)\n",
    "            \n",
    "            print(f\"Loaded dataset from {file_path}: {len(df)} rows\")\n",
    "            dataframes.append(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Comprehensive disaster risk prediction pipeline\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load all datasets from current directory\n",
    "        dataframes = load_all_datasets()\n",
    "        \n",
    "        # Check if datasets were loaded\n",
    "        if not dataframes:\n",
    "            raise ValueError(\"No datasets found. Please ensure CSV or XLSX files are present.\")\n",
    "        \n",
    "        # Initialize preprocessor with all datasets\n",
    "        preprocessor = ComprehensiveDisasterRiskPreprocessor(dataframes)\n",
    "        \n",
    "        # Extract and engineer features\n",
    "        X, y = preprocessor.select_and_engineer_features()\n",
    "        \n",
    "        # Train advanced model\n",
    "        model, scaler, label_encoder, training_diagnostics = train_advanced_disaster_model(X, y)\n",
    "        \n",
    "        # Split data for final evaluation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=0.2, \n",
    "            random_state=42, \n",
    "            stratify=y\n",
    "        )\n",
    "        \n",
    "        # Generate comprehensive plots\n",
    "        generate_comprehensive_plots(\n",
    "            training_diagnostics, \n",
    "            preprocessor, \n",
    "            model, \n",
    "            scaler.transform(X_test), \n",
    "            y_test, \n",
    "            label_encoder\n",
    "        )\n",
    "        \n",
    "        # Save model components for deployment\n",
    "        save_model_components(\n",
    "            model, \n",
    "            scaler, \n",
    "            label_encoder, \n",
    "            preprocessor,\n",
    "            save_path='risk_model.pkl'\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(\"An error occurred:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3bfb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
